{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "epsilon = 1e-9\n",
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda_val = 0.5\n",
    "reg_scale = 0.392   # 0.0005 * 784\n",
    "\n",
    "class CapsNetModel(object):\n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "    def _squash(self, vector):\n",
    "        vec_abs = tf.sqrt(tf.reduce_sum(tf.square(vector)))\n",
    "        scalar = tf.square(vec_abs) / (1 + tf.square(vec_abs))\n",
    "        squashed_vector = scalar * tf.divide(vector, vec_abs)\n",
    "        return squashed_vector\n",
    "\n",
    "    def _routing(self, input, b):\n",
    "        W = tf.get_variable('Weight', shape=(1, 1152, 10, 8, 16), dtype=tf.float32,\n",
    "                            initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        input = tf.tile(input, [1, 1, 10, 1, 1])\n",
    "        W = tf.tile(W, [self.batch_size, 1, 1, 1, 1])\n",
    "        u_hat = tf.matmul(W, input, transpose_a=True)\n",
    "        u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')\n",
    "\n",
    "        iter_routing = 3\n",
    "        for r_iter in range(iter_routing):\n",
    "            with tf.variable_scope('iter_' + str(r_iter)):\n",
    "                # line 4:\n",
    "                # => [1, 1152, 10, 1, 1]\n",
    "                c_IJ = tf.nn.softmax(b, dim=2)\n",
    "\n",
    "                # At last iteration, use `u_hat` in order to receive gradients from the following graph\n",
    "                if r_iter == iter_routing - 1:\n",
    "                    # line 5:\n",
    "                    # weighting u_hat with c_IJ, element-wise in the last two dims\n",
    "                    # => [batch_size, 1152, 10, 16, 1]\n",
    "                    s_J = tf.multiply(c_IJ, u_hat)\n",
    "                    # then sum in the second dim, resulting in [batch_size, 1, 10, 16, 1]\n",
    "                    s_J = tf.reduce_sum(s_J, axis=1, keep_dims=True)\n",
    "                    assert s_J.get_shape() == [self.batch_size, 1, 10, 16, 1]\n",
    "\n",
    "                    # line 6:\n",
    "                    # squash using Eq.1,\n",
    "                    v_J = self._squash(s_J)\n",
    "                    assert v_J.get_shape() == [self.batch_size, 1, 10, 16, 1]\n",
    "                elif r_iter < iter_routing - 1:  # Inner iterations, do not apply backpropagation\n",
    "                    s_J = tf.multiply(c_IJ, u_hat_stopped)\n",
    "                    s_J = tf.reduce_sum(s_J, axis=1, keep_dims=True)\n",
    "                    v_J = self._squash(s_J)\n",
    "\n",
    "                    # line 7:\n",
    "                    # reshape & tile v_j from [batch_size ,1, 10, 16, 1] to [batch_size, 1152, 10, 16, 1]\n",
    "                    # then matmul in the last tow dim: [16, 1].T x [16, 1] => [1, 1], reduce mean in the\n",
    "                    # batch_size dim, resulting in [1, 1152, 10, 1, 1]\n",
    "                    v_J_tiled = tf.tile(v_J, [1, 1152, 1, 1, 1])\n",
    "                    u_produce_v = tf.matmul(u_hat_stopped, v_J_tiled, transpose_a=True)\n",
    "                    assert u_produce_v.get_shape() == [self.batch_size, 1152, 10, 1, 1]\n",
    "\n",
    "                    # b_IJ += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n",
    "                    b += u_produce_v\n",
    "\n",
    "        return (v_J)\n",
    "\n",
    "    def build_graph(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # input place holders\n",
    "        self.X = tf.placeholder(tf.float32, [batch_size, 784], name=\"INPUT_IMAGE\")\n",
    "\n",
    "        # img 28x28x1 (black/white), Input Layer\n",
    "        X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "        #self.labels = tf.placeholder(tf.float32, shape=[batch_size * 10])\n",
    "        #self.Y = tf.placeholder(tf.float32, [None, 10])\n",
    "        #self.Y = tf.reshape(self.labels, shape=(-1, 10, 1))\n",
    "        self.Y = tf.placeholder(tf.float32, [batch_size, 10])\n",
    "        self.labels = tf.to_int32(tf.argmax(self.Y, axis=1))\n",
    "\n",
    "        # Convolutional Layer\n",
    "        conv1 = tf.layers.conv2d(inputs=X_img, filters=256, kernel_size=[9, 9],\n",
    "                                 padding=\"VALID\", activation=tf.nn.relu)\n",
    "\n",
    "        # Capsule 1\n",
    "        caps = []\n",
    "        for i in range(8):\n",
    "            capsule = tf.layers.conv2d(inputs=conv1, filters=32, kernel_size=[9, 9],\n",
    "                                        padding=\"VALID\", strides=2, activation=None)\n",
    "            capsule = tf.reshape(capsule, (-1, 1152, 1, 1))\n",
    "            caps.append(capsule)\n",
    "        temp = tf.reshape(caps, (-1, 1152, 1, 1, 1))\n",
    "        caps = tf.concat(caps, axis=2)\n",
    "        caps = self._squash(caps)\n",
    "\n",
    "        # Capsule 2\n",
    "        # b.shape = (-1, 1152, 10, 1, 1)\n",
    "        #b = tf.zeros_like(tf.tile(temp, [1, 1, 10, 1, 1]))\n",
    "        b = tf.constant(np.zeros([batch_size, 1, 10, 1, 1], dtype=np.float32))\n",
    "        recaps = tf.reshape(caps, (-1, 1152, 1, 8, 1))\n",
    "\n",
    "        caps2 = self._routing(recaps, b)\n",
    "        caps2 = tf.squeeze(caps2, axis=1)\n",
    "\n",
    "        self.v_len = tf.sqrt(tf.reduce_sum(tf.square(caps2), axis=2, keep_dims=True) + epsilon)\n",
    "        sm = tf.nn.softmax(self.v_len, dim=1)\n",
    "\n",
    "\n",
    "        # result\n",
    "        argmax_index = tf.to_int32(tf.argmax(sm, axis=1))\n",
    "        self.argmax_index = tf.reshape(argmax_index, (self.batch_size, ), name=\"OUTPUT\")\n",
    "        #\n",
    "\n",
    "        masked = []\n",
    "        for batch_size in range(self.batch_size):\n",
    "            v = caps2[batch_size][self.argmax_index[batch_size], :]\n",
    "            masked.append(tf.reshape(v, shape=(1, 1, 16, 1)))\n",
    "        self.masked = tf.concat(masked, axis=0)\n",
    "        #\n",
    "        #self.masked = tf.multiply(tf.squeeze(caps2), tf.reshape(self.Y, (-1, 10, 1)))\n",
    "        #\n",
    "\n",
    "        # reconstruction\n",
    "        vector_j = tf.reshape(self.masked, shape=(self.batch_size, -1))\n",
    "        fc1 = tf.contrib.layers.fully_connected(vector_j, num_outputs=512)\n",
    "        assert fc1.get_shape() == [self.batch_size, 512]\n",
    "        fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=1024)\n",
    "        assert fc2.get_shape() == [self.batch_size, 1024]\n",
    "        self.decoded = tf.contrib.layers.fully_connected(fc2, num_outputs=784, activation_fn=tf.sigmoid)\n",
    "\n",
    "        # loss\n",
    "        max_l = tf.square(tf.maximum(0., m_plus - self.v_len))\n",
    "        max_r = tf.square(tf.maximum(0., self.v_len - m_minus))\n",
    "\n",
    "        max_l = tf.reshape(max_l, shape=(self.batch_size, -1))\n",
    "        max_r = tf.reshape(max_r, shape=(self.batch_size, -1))\n",
    "\n",
    "        #T_c = tf.reshape(self.Y, (-1, 10))\n",
    "        T_c = self.Y\n",
    "        L_c = T_c * max_l + lambda_val * (1 - T_c) * max_r\n",
    "\n",
    "        self.loss = tf.reduce_mean(tf.reduce_sum(L_c, axis=1))\n",
    "\n",
    "        # reconstruction loss\n",
    "        origin = tf.reshape(self.X, shape=(self.batch_size, -1))\n",
    "        squared = tf.square(self.decoded - origin)\n",
    "        self.reconstruction_err = tf.reduce_mean(squared)\n",
    "\n",
    "        # total loss\n",
    "        self.total_loss = self.loss + reg_scale * self.reconstruction_err\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "        self.train_op = self.optimizer.minimize(self.total_loss)  # var_list=t_vars)\n",
    "\n",
    "        correct_prediction = tf.equal(tf.to_int32(self.labels), self.argmax_index)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        return self.sess.run(self.argmax_index,\n",
    "                             feed_dict={self.X: x_test})\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test):\n",
    "        return self.sess.run(self.accuracy,\n",
    "                             feed_dict={self.X: x_test,\n",
    "                                        self.Y: y_test})\n",
    "\n",
    "    def train(self, x_data, y_data):\n",
    "        return self.sess.run([self.total_loss, self.train_op], feed_dict={\n",
    "            self.X: x_data, self.Y: y_data})\n",
    "    \n",
    "    def save(self, filename):\n",
    "        tf.train.Saver().save(self.sess, filename)\n",
    "        \n",
    "    def restore(self, filename):\n",
    "        tf.train.Saver().restore(self.sess, filename)\n",
    "        \n",
    "    def export_graph(self, output_node_name_list, filename):\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            self.sess, # The session is used to retrieve the weights\n",
    "            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n",
    "            output_node_name_list\n",
    "        ) \n",
    "        # Finally we serialize and dump the output graph to the filesystem\n",
    "        with tf.gfile.GFile(filename, \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = CapsNetModel()\n",
    "m1.build_graph(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 15\n",
    "\n",
    "print('Learning Started!')\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        print(i, '/', total_batch)\n",
    "        c, _ = m1.train(batch_xs, batch_ys)\n",
    "        avg_cost += c / total_batch\n",
    "    \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "total_batch = int(mnist.test.num_examples / batch_size)\n",
    "batch_xs, batch_ys = mnist.test.next_batch(batch_size)\n",
    "print('Accuracy:', m1.get_accuracy(batch_xs, batch_ys))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch = int(mnist.test.num_examples / batch_size)\n",
    "batch_xs, batch_ys = mnist.test.next_batch(batch_size)\n",
    "print('Accuracy:', m1.get_accuracy(batch_xs, batch_ys))\n",
    "\n",
    "print('predict')\n",
    "results = m1.predict(batch_xs)\n",
    "for i in range(30):\n",
    "    print(results[i], batch_ys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.save('./caps_mnist.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.restore('./caps_mnist.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.export_graph(['OUTPUT'], 'frozen_graph.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
